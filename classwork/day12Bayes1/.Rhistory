knitr::opts_chunk$set(echo = TRUE)
S <- cbind(c(4,1.2,1.2),c(1.2,4,1.2),c(1.2,1.2,4))
set.seed(1934)
y <- rnorm(3)
multinorm <- function(mu, y, iter, Sigma){
S <- Sigma
s <- S [1,-1]%*%solve(S[-1,-1])%*%S[-1,1]
M <- S[1,-1]%*%solve(S[-1,-1])
y.final <- NULL
for (i in 1:iter){
y[1] <- rnorm(1, mu[1]+M%*%(y[-1]-c(mu[2],mu[3])), sqrt(4-s))
y[2] <- rnorm(1, mu[2]+M%*%(y[-2]-c(mu[1],mu[3])), sqrt(4-s))
y[3] <- rnorm(1, mu[3]+M%*%(y[-3]-c(mu[1],mu[2])), sqrt(4-s))
y.final <- rbind(y.final, y)
}
return(y.final)
}
out <- multinorm(mu=c(1,2,3), y=y, iter=15000, Sigma=S)
out <- out[-(1:5000),]
out2 <- out[seq(1,10000,by=10),]
par(mfrow=c(3,1))
acf(out2[,1], main=expression(y[1]))
acf(out2[,1], main=expression(y[1]))
acf(out2[,2], main=expression(y[2]))
acf(out2[,3], main=expression(y[3]))
library(coda)
mcmc.object <- mcmc(out, start=5001, end=15000, thin=10)
summary(mcmc.object)
traceplot(mcmc.object)
library(LearnBayes)
set.seed(1919)
log.post <- function(data, params){
sum(sapply(1:length(data$data), function(x) y[x]*(params[1] + (params[2]*x))-exp(params[1]+(params[2]*x))))
}
prop.f<-function(data){
laplace(log.post, mode=start, data=data)$mode
}
met <- function(data, start, iters){
chain <- array(dim = c(iters+1,length(start)))
chain[1,] <- start
for (i in 1:iters){
proposal <- prop.f(data)
prob <- exp(log.post(data, proposal)) - log.post(data,chain[i,])
if (runif(1) < prob){
chain[i+1,] <- proposal
}else{
chain[i+1,] <- chain[i,]
}
}
return(chain)
}
y <- c(15,11,14,17,5,11,10,4,8,10,7,9,11,3,6,1,1,4)
start <- rnorm(2)
data <- list(data=y, par=start)
out <- met(data,start, 1000)
mean(out[,2])
sd(out[,2])
#lambdas
exp(mean(out[,1])+mean(out[,2])*1:18)
theta <- seq(0,1,by=.001)
plot(theta, dbeta(theta,1,2/3), type='l', ylab='density', ylim=c(0,30))
lines(theta, dbeta(theta, 651, 350+2/3), lty=2)
plot(theta, dbeta(theta,1,2/3), type='l', ylab='density', ylim=c(0,30))
lines(theta, dbeta(theta, 651, 350+2/3), lty=2)
legend('topleft', c('prior', 'posterior'), lty=c(1,2))
